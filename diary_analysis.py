from transformers import pipeline
import pytextrank
import spacy


class DiarySentimentAnalysis:
    def split_sentence(self, input_text: str):
        """Splits the input text into sentences."""
        self.seper_senten = input_text.split('.')
        self.seper_senten = [sentence.strip() for sentence in self.seper_senten if sentence.strip()]
        return self.seper_senten
    
    def create_emotion_analyzer(self):
        return pipeline("text-classification", model="beomi/kcbert-base")

    def analyze_diary_entry(self,sentences, emotion_analyzer):
        # ê°ì • -> ì´ëª¨ì§€ ë§¤í•‘
        emotion_to_emoji = {
            "ê³µí¬": "ğŸ˜±",
            "ë†€ëŒ": "ğŸ˜²",
            "ë¶„ë…¸": "ğŸ˜¡",
            "ìŠ¬í””": "ğŸ˜¢",
            "ì¤‘ë¦½": "ğŸ˜",
            "í–‰ë³µ": "ğŸ˜Š",
            "í˜ì˜¤": "ğŸ¤¢"
        }

        result = []
        for sentence in sentences:

            # ê°ì • ë¶„ì„ ìˆ˜í–‰
            emotion_result = emotion_analyzer(sentence)[0]
            emotion = emotion_result["label"]
            emoji = emotion_to_emoji.get(emotion, "ğŸ¤”")  # ë§¤í•‘ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ì´ëª¨ì§€ ì‚¬ìš©
            result.append(emoji)
        return result


    ### ê°ì • ë¶„ë¥˜ì‹œ ì¤‘ë¦½ ì œì™¸ ì¸ë±ìŠ¤ë¡œ ìˆ˜ì • ##
    def filter_sementic(self, result: list[str]):
        """Filters sentences based on important emotion labels and combines them with previous sentences."""
        important_labels = {"ğŸ˜±", "ğŸ˜²", "ğŸ˜¡", "ğŸ˜¢", "ğŸ˜Š", "ğŸ¤¢","ğŸ¤”"}  # Labels of interest
        filter_seper_senten = [self.seper_senten[i] for i, label in enumerate(result) if label in important_labels]
        filter_seper_label = [result[i] for i, label in enumerate(result) if label in important_labels]

        # Combine each selected sentence with the previous one
        filter_seper_senten2 = []
        for item in filter_seper_senten:
            index = self.seper_senten.index(item)
            if index > 0:
                combined_sentence = f'{self.seper_senten[index - 1]} {item}'
                filter_seper_senten2.append(combined_sentence)
            else:
                filter_seper_senten2.append(item)

        return filter_seper_senten, filter_seper_label
    
    def get_only_pred_labels_filterd(self, pred_result: list[str]) -> list[str]:
        a, filtered_pred_labels = self.filter_sementic(pred_result)
        return filtered_pred_labels
    
    def keyword_extract_f(self, sentences):
        keyword = []
        nlp = spacy.load("en_core_web_sm")
        nlp.add_pipe("textrank")

        for sentence in sentences:
            doc = nlp(sentence)
            keyword_sourece = [phrase.text for phrase in doc._.phrases[:1]]
            print()
            keyword.append(keyword_sourece)
        return keyword

    def __call__(self, input_text: str):
        """Runs the full analysis when the class is called."""
        # Split sentences
        self.split_sentence(input_text=input_text)

        # Perform semantic classification
        emotion_analyzer = self.create_emotion_analyzer()
        result_s_model = self.analyze_diary_entry(self.seper_senten, emotion_analyzer)

        # Filter sentences
        filter_seper_senten, pred_labels_filterd = self.filter_sementic(result_s_model)

        # Extract keywords
        result_k_model = self.keyword_extract_f(filter_seper_senten)

        # Output results
        print("Separated Sentences:", self.seper_senten)
        print("sentimental classificatioin per sentence:", result_s_model)
        print("Filtered Sentences:", filter_seper_senten)
        print("Filtered sentiment:", pred_labels_filterd)
        print("Keywords:", result_k_model)
        return filter_seper_senten, pred_labels_filterd, result_k_model
    
    
if __name__ == "__main__":
    from diaryanaly import DiarySentimentAnalysis
    diary_analyzer = DiarySentimentAnalysis()

    test_case_1 = "ì•ˆë…•í•˜ì„¸ìš”~ ì˜¤ëŠ˜ì€ ì§„ì§œ ì¢‹ì€ì¼ì´ìˆì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ì•ˆì¢‹ì€ì¼ë„ìˆì—ˆì–´ìš”. ì½”ë“œë¥¼ ì§œëŠ”ë° ì§‘ì¤‘ì´ í•˜ë‚˜ë„ì•ˆë˜ë„¤.."
    test_case_2 = "ì˜¤ëŠ˜ì€ ì •ì‹ ì´ í˜¼ë€ìŠ¤ëŸ¬ìš´ í•˜ë£¨ì˜€ë‹¤. ìƒì‚°ì§ì´ë‚˜ ì•Œë°”, ë¬´ìŠ¨ ì¼ì´ë“  í•˜ë ¤ëŠ”ë° ì—¬ëŸ¬ ì§ì¥ì—ì„œì˜ ì „í™”ë¡œ ì •ì‹ ì—†ì´ ë°”ìœ í•˜ë£¨ë¥¼ ë³´ëƒˆë‹¤. ë¬¼ë¡  ì—…ë¬´ë„ ë°”ë¹ ì„œ ìŠ¹ê¸‰ ì‹œí—˜ê³¼ ì œì¶œ ê¸°í•œì´ ì½”ì•ì— ë‹¤ê°€ì™€ ìˆì–´ì„œ ë”ìš± ê¸´ì¥ë˜ëŠ” ìƒí™©ì´ì—ˆë‹¤. ëŠ¦ì ì„ ìì„œ íšŒì‚¬ì— ëŠ¦ê²Œ ê°€ì•¼ í–ˆê³ , ì¹œêµ¬ì˜ í´ëŸ½ ëª¨ì„ ì‚¬ì‹¤ì„ ì•Œê²Œ ë˜ì–´ ë‹¹í™©ìŠ¤ëŸ¬ìš´ ì¼ë„ ìˆì—ˆë‹¤."
    
    test_case = test_case_2  # shadowing (act as a switch)
    
    filter_seper_senten, result_s_model, result_k_model = \
        diary_analyzer(test_case)
    
    custom_test_result = diary_analyzer.get_only_pred_labels_filterd(test_case)
    print(f"custom test result - {custom_test_result}")
# end main